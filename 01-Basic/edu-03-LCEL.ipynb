{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기본 예시 : 프롬프트 + 모델 + 출력파서\n",
    "\n",
    "`prompt` + `model` + `parser`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith 추적을 시작합니다.\n",
      "[프로젝트명]\n",
      "edu-CH01-Basic\n"
     ]
    }
   ],
   "source": [
    "from langchain_teddynote import logging\n",
    "\n",
    "logging.langsmith('edu-CH01-Basic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 프롬프트 템플릿의 활용\n",
    "\n",
    "`promptTemplate`\n",
    "\n",
    "`input_variables`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.messages import stream_response    # 스트리밍 출력\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`from_template()` 메소드를 사용하여 PromptTemplate 객체 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['country'], input_types={}, partial_variables={}, template='{country}의 수도는 어디인가요?')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# template 정의 \n",
    "template = \"{country}의 수도는 어디인가요?\"\n",
    "\n",
    "# from_template 메소드를 이용하여 PromptTemplate 객체 생성 \n",
    "propmpt_template = PromptTemplate.from_template(template=template)\n",
    "propmpt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'대한민국의 수도는 어디인가요?'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt 생성\n",
    "prompt = propmpt_template.format(country=\"대한민국\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'미국의 수도는 어디인가요?'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt 생성\n",
    "prompt = propmpt_template.format(country=\"미국\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain 생성\n",
    "\n",
    "LCEL(LangChain Expression Language)\n",
    "\n",
    "`chain = prompt | model | output_parser`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt를 PromptTemplate 객체로 생성\n",
    "# prompt = PromptTemplate.from_template(\"{topic} 에 대해 {how} 설명해주세요.\")\n",
    "prompt = PromptTemplate.from_template(\"{topic} 에 대해 쉽게 설명해주세요.\")\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.1)\n",
    "\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['how', 'topic'], input_types={}, partial_variables={}, template='{topic} 에 대해 {how} 설명해주세요.')\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001494024E350>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001494027E750>, root_client=<openai.OpenAI object at 0x00000149402569D0>, root_async_client=<openai.AsyncOpenAI object at 0x000001494024E510>, model_name='gpt-4o-mini', temperature=0.1, model_kwargs={}, openai_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "invoke() 호출\n",
    "\n",
    "`{key: value}` 딕셔너리 형태로 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input 딕셔너리에 주제를 '인공지능 모델의 학습 원리'로 설정 \n",
    "# input = {\"topic\": \"인공지능 모델의 학습 원리\", \"how\": \"간단하게\"}\n",
    "input = {\"topic\": \"인공지능 모델의 학습 원리\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt 객체와 model 객체를 파이브(|)로 연결, invoke 메서드를 사용하여 input 전달.\n",
    "chain.invoke(input)\n",
    "\n",
    "# 인자가 1개인 경우는 input을 딕셔너리로 넣지 않아도 오류가 발생하지 않음. \n",
    "answer = chain.invoke(\"인공지능의 학습원리\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인공지능 모델의 학습 원리를 쉽게 설명하자면, 다음과 같은 단계로 이해할 수 있습니다.\n",
      "\n",
      "1. **데이터 수집**: 인공지능 모델은 학습하기 위해 많은 데이터를 필요로 합니다. 이 데이터는 이미지, 텍스트, 소리 등 다양한 형태일 수 있습니다.\n",
      "\n",
      "2. **데이터 전처리**: 수집한 데이터는 모델이 이해할 수 있는 형태로 가공해야 합니다. 예를 들어, 이미지의 크기를 조정하거나, 텍스트를 숫자로 변환하는 등의 작업이 필요합니다.\n",
      "\n",
      "3. **모델 선택**: 학습할 모델을 선택합니다. 이는 신경망, 결정 트리, 서포트 벡터 머신 등 여러 종류가 있으며, 문제의 특성에 따라 적합한 모델을 선택합니다.\n",
      "\n",
      "4. **학습**: 모델은 주어진 데이터를 바탕으로 패턴을 학습합니다. 이 과정에서 모델은 입력 데이터와 정답(라벨) 간의 관계를 파악하게 됩니다. 예를 들어, 고양이와 개의 이미지를 분류하는 모델은 각각의 특징을 학습하게 됩니다.\n",
      "\n",
      "5. **손실 함수**: 모델의 예측이 얼마나 정확한지를 평가하기 위해 손실 함수를 사용합니다. 손실 함수는 모델의 예측값과 실제값 간의 차이를 수치적으로 나타내줍니다.\n",
      "\n",
      "6. **최적화**: 모델의 예측 정확도를 높이기 위해 가중치를 조정합니다. 이 과정은 반복적으로 이루어지며, 경량화된 방법(예: 경사 하강법)을 사용하여 손실을 최소화합니다.\n",
      "\n",
      "7. **검증**: 학습이 끝난 후, 모델의 성능을 평가하기 위해 검증 데이터를 사용합니다. 이 데이터는 모델이 학습할 때 사용하지 않은 데이터로, 모델의 일반화 능력을 확인하는 데 도움을 줍니다.\n",
      "\n",
      "8. **배포 및 사용**: 모델이 충분히 학습하고 검증되면 실제 환경에 배포되어 사용됩니다. 이후에도 새로운 데이터로 모델을 업데이트하거나 개선할 수 있습니다.\n",
      "\n",
      "이러한 과정을 통해 인공지능 모델은 주어진 문제를 해결할 수 있는 능력을 갖추게 됩니다."
     ]
    }
   ],
   "source": [
    "# 스트리밍 출력\n",
    "answer = chain.stream(input)\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'completion_tokens': 386,\n",
       "  'prompt_tokens': 21,\n",
       "  'total_tokens': 407,\n",
       "  'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
       "   'audio_tokens': 0,\n",
       "   'reasoning_tokens': 0,\n",
       "   'rejected_prediction_tokens': 0},\n",
       "  'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}},\n",
       " 'model_name': 'gpt-4o-mini-2024-07-18',\n",
       " 'system_fingerprint': 'fp_b8bc95a0ac',\n",
       " 'id': 'chatcmpl-BEvAUHHUOxXKYVXHV9gulNUYJ1zcJ',\n",
       " 'finish_reason': 'stop',\n",
       " 'logprobs': None}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer.content\n",
    "answer.response_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 출력 파서 (Output Parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chain에 출력파서를 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프롬프트, 모델, 출력 파서를 연결하여 처리 체인을 구성 \n",
    "chain = prompt | model | output_parser\n",
    "# chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'인공지능 모델의 학습 원리를 쉽게 설명하자면, 다음과 같은 단계로 나눌 수 있습니다.\\n\\n1. **데이터 수집**: 인공지능 모델은 학습하기 위해 많은 데이터를 필요로 합니다. 이 데이터는 이미지, 텍스트, 소리 등 다양한 형태일 수 있습니다. 예를 들어, 고양이와 개를 구분하는 모델을 만들고 싶다면, 고양이와 개의 사진이 포함된 데이터셋이 필요합니다.\\n\\n2. **데이터 전처리**: 수집한 데이터는 종종 정리하고 가공해야 합니다. 예를 들어, 이미지의 크기를 통일하거나, 텍스트에서 불필요한 정보를 제거하는 과정이 필요합니다.\\n\\n3. **모델 선택**: 학습할 모델의 구조를 선택합니다. 이는 신경망, 결정 트리, 서포트 벡터 머신 등 다양한 형태가 있을 수 있습니다. 각 모델은 특정한 문제에 더 적합할 수 있습니다.\\n\\n4. **학습**: 모델은 데이터를 통해 패턴을 학습합니다. 이 과정에서 모델은 입력 데이터(예: 고양이 사진)를 받아들이고, 그에 대한 정답(예: \"고양이\")과 비교하여 얼마나 잘 맞추는지를 평가합니다. 이 평가 결과를 바탕으로 모델의 내부 파라미터를 조정하여 점점 더 정확하게 예측할 수 있도록 합니다.\\n\\n5. **검증**: 학습이 끝난 후, 모델의 성능을 평가하기 위해 새로운 데이터(훈련에 사용하지 않은 데이터)를 사용합니다. 이를 통해 모델이 실제 상황에서도 잘 작동하는지 확인합니다.\\n\\n6. **배포 및 활용**: 모델이 충분히 잘 학습되었다면, 실제 애플리케이션에 배포하여 사용합니다. 예를 들어, 고양이와 개를 구분하는 앱에 적용할 수 있습니다.\\n\\n이 과정을 반복하면서 모델은 점점 더 정확해지고, 다양한 문제를 해결할 수 있는 능력을 갖추게 됩니다.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chain 객체의 invoke 메서드를 사용하여 input 전달 \n",
    "input = {\"topic\": \"인공지능 모델의 학습 원리\"}\n",
    "chain.invoke(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스트리밍 출력\n",
    "answer = chain.stream(input)\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 템플릿을 변경하여 적용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "당신은 영어를 가르치는 10년차 영어 선생님입니다. 주어진 상황에 맞는 영어 회화를 작성해 주세요.\n",
    "양식은 [FORMAT]을 참고하여 작성해 주세요.\n",
    "\n",
    "#상황:\n",
    "{question}\n",
    "\n",
    "#FORMAT:\n",
    "- 영어 회화:\n",
    "- 한글 해석:\n",
    "\"\"\"\n",
    "\n",
    "# 프롬프트 템플릿을 이용하여 프롬프트를 생성\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "prompt\n",
    "\n",
    "# ChatOpenAI 모델 초기화\n",
    "model = ChatOpenAI(model_name=\"gpt-4o-mini\")\n",
    "\n",
    "# 문자열 출력 파서 초기화 \n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 체인 구성 \n",
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 영어 회화:\n",
      "  A: Hi there! Welcome to our restaurant. How can I help you today?\n",
      "  B: Hi! I’d like to see the menu, please.\n",
      "  A: Sure! Here you go. Our specials today are the grilled salmon and the pasta primavera.\n",
      "  B: That sounds delicious! I think I’ll have the grilled salmon.\n",
      "  A: Great choice! Would you like anything to drink?\n",
      "  B: Yes, I’ll have a glass of iced tea, please.\n",
      "  A: Perfect! Your order will be ready shortly. Enjoy your meal!\n",
      "\n",
      "- 한글 해석:\n",
      "  A: 안녕하세요! 저희 식당에 오신 것을 환영합니다. 오늘 무엇을 도와드릴까요?\n",
      "  B: 안녕하세요! 메뉴를 보고 싶어요.\n",
      "  A: 물론입니다! 여기 있습니다. 오늘의 특별 메뉴는 구운 연어와 파스타 프리마베라입니다.\n",
      "  B: 맛있을 것 같아요! 구운 연어를 먹겠습니다.\n",
      "  A: 좋은 선택이에요! 음료는 무엇을 드릴까요?\n",
      "  B: 네, 아이스티 한 잔 주세요.\n",
      "  A: 완벽해요! 주문하신 음식은 곧 준비될 것입니다. 맛있게 드세요!\n"
     ]
    }
   ],
   "source": [
    "# 완성된 Chain을 실행하여 답변을 얻음. \n",
    "print(chain.invoke({\"question\": \"저는 식당에 가서 음식을 주문하고 싶어요\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스트리밍 출력\n",
    "answer = chain.stream({\"question\": \"저는 식당에 가서 음식을 주문하고 싶어요\"})\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 영어 회화:\n",
      "A: Hi! I'd like to place an order for a pizza, please.  \n",
      "B: Sure! What size would you like?  \n",
      "A: I'll have a large, please.  \n",
      "B: Great! What toppings do you want?  \n",
      "A: I'd like pepperoni and mushrooms, please.  \n",
      "B: Perfect choice! Would you like anything to drink?  \n",
      "A: Yes, a 2-liter bottle of soda, please.  \n",
      "B: Sounds good! Can I have your address for delivery?  \n",
      "A: Sure, it's 123 Maple Street.  \n",
      "B: Thank you! Your total comes to $25. Would you like to pay with cash or card?  \n",
      "A: I'll pay with a card.  \n",
      "B: Great! Your pizza will be delivered in about 30 minutes.  \n",
      "A: Thank you!\n",
      "\n",
      "- 한글 해석:\n",
      "A: 안녕하세요! 피자 주문하고 싶어요.  \n",
      "B: 네, 물론이죠! 어떤 사이즈로 드릴까요?  \n",
      "A: 큰 사이즈로 주세요.  \n",
      "B: 좋습니다! 어떤 토핑을 원하시나요?  \n",
      "A: 페퍼로니와 버섯으로 해주세요.  \n",
      "B: 완벽한 선택이에요! 음료수도 필요하신가요?  \n",
      "A: 네, 2리터 소다 한 병 주세요.  \n",
      "B: 좋습니다! 배달할 주소는 어떻게 되시나요?  \n",
      "A: 네, 123 메이플 스트리트입니다.  \n",
      "B: 감사합니다! 총 금액은 25달러입니다. 현금으로 결제하시겠어요, 아니면 카드로 하시겠어요?  \n",
      "A: 카드로 결제할게요.  \n",
      "B: 좋습니다! 피자는 약 30분 후에 배달됩니다.  \n",
      "A: 감사합니다!"
     ]
    }
   ],
   "source": [
    "answer = chain.stream({\"question\": \"미국에서 피자 주문\"})\n",
    "# answer = chain.stream(\"미국에서 피자 주문\") # 인자가 1개이면 딕셔너리 생략 가능\n",
    "stream_response(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-kr-UwKF-Wpu-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
